# -*- coding: utf-8 -*-
"""MLE_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g3phFpeGuN6wqjqbIuP2ZmbnjN-pSt23
"""

import os
import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Initialising.....")

data = pd.read_csv('/content/kc_house_data.csv')

data.head()

"""# **EDA**"""

logger.info(f"Dataset Shape: {data.shape}")
logger.info(f"Dataset Sample:\n{data.head()}")

logger.info("Performing Basic EDA...")

data.info()

data.describe()

"""1. The dataset has 21 columns and 21,613 observations.
2. There are no missing values in any of the columns, which simplifies cleaning. The average house price sits around 540,000 dollars, and the maximum price is 7.7 million dollars, there is a wide range of property values.
3. Bedrooms range from 0 to 33, but most homes have 3 to 4 bedrooms.
4. The average number of bathrooms is about 2. 5. The median living area is 1,910 sq ft.
6. The mean year built is 1971, with renovation years spanning up to 2015.
"""

print(data.isnull().sum())

"""Price, Bedrooms and sqft_living seem suspicious, might have outliers

Confirms that there are no null values.
"""

features_to_check = ['price', 'bedrooms', 'sqft_living']

# Boxplots
for feature in features_to_check:
    plt.figure(figsize=(6, 4))
    plt.boxplot(data[feature], vert=False)
    plt.title(f'Boxplot of {feature}')
    plt.xlabel(feature)
    plt.show()

# Histograms
for feature in features_to_check:
    plt.figure(figsize=(6, 4))
    plt.hist(data[feature], bins=30)
    plt.title(f'Histogram of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Count')
    plt.show()

"""Nothing too extreme here"""

plt.figure(figsize=(10, 8))
corr_matrix = data.select_dtypes(include=np.number).corr()
sns.heatmap(corr_matrix, annot=False, cmap='YlGnBu')
plt.title('Correlation Heatmap')
plt.show()

corr_with_price = corr_matrix['price'].abs().sort_values(ascending=False)
print(corr_with_price)

data.hist(bins=20,figsize=(20,20))

"""From this we can conclude we should choose only a few features, as not all provide good correlation with fluctuating price."""

data_subset = data[ ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built']   ]

data_subset.info()

sns.pairplot(data_subset)

"""we can further remove year built, as it is not affecting price."""

data_subset = data_subset.drop('yr_built', axis=1)

"""Now, we can seperate the Features and the Target"""

X = data_subset.drop('price', axis=1)
y = data_subset['price']

X

X.shape

y.shape

from sklearn.preprocessing import MinMaxScaler
scaler=StandardScaler()
X_scaled= scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

logger.info("Data split into train and test sets.")

!pip install xgboost
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Dictionary of baseline models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42)
}

results = {}  # to store the performance of each model

for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on test set
    y_pred = model.predict(X_test)

    # Evaluate
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    # Print performance
    print(f"{model_name} Results:")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  MAE:  {mae:.2f}")
    print(f"  R2:   {r2:.2f}")
    print("-" * 30)

    # Store in results dict
    results[model_name] = (rmse, mae, r2)

logger.info("Starting GridSearchCV for RandomForest...")

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
}

rf = RandomForestRegressor(random_state=42)

grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=3,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

logger.info(f"Best params for RF:", grid_search.best_params_)
print("Best Score (MSE):", grid_search.best_score_)

best_rf_model = grid_search.best_estimator_

best_rmse = np.sqrt(-grid_search.best_score_)
print(f"Best RMSE: {best_rmse:.2f}")



y_pred_best = best_rf_model.predict(X_test)

# Calculate Metrics
rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))
mae_best = mean_absolute_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)
print("Tuned Random Forest Model Performance on Test Set:")
print(f"  RMSE: {rmse_best:.2f}")
print(f"  MAE:  {mae_best:.2f}")
print(f"  R²:   {r2_best:.2f}")

from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': [100, 300],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

xgb = XGBRegressor(random_state=42)
random_search = RandomizedSearchCV(  estimator=xgb, param_distributions=param_dist,  scoring='neg_mean_squared_error', n_iter=10,   cv=3,  n_jobs=-1,  verbose=1, random_state=42)
random_search.fit(X_train, y_train)

print("Best Parameters (XGB):", random_search.best_params_)
print("Best Score (XGB MSE):", random_search.best_score_)

best_xgb_model = random_search.best_estimator_

y_pred_xgb = best_xgb_model.predict(X_test)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print("Tuned XGB Model Performance:")
print(f"  RMSE: {rmse_xgb:.2f}")
print(f"  MAE: {mae_xgb:.2f}")
print(f"  R²:  {r2_xgb:.2f}")

import pickle
with open('best_rf_model.pkl', 'wb') as f:
    pickle.dump(best_rf_model, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

